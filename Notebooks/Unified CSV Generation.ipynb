{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to obtain the unique headers amongst all the csv files that we are going to merge. Then we will finally make a master comma seperated value file with list of headers that we obtain from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parent directory and sub directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"../UnifiedCSVTestData/\"\n",
    "filelist = []\n",
    "dirs = []\n",
    "\n",
    "def makefilelist(parent_dir):\n",
    "    headers = []\n",
    "    csv_headers = []\n",
    "    subject_dirs = [os.path.join(parent_dir, dir) for dir in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, dir))]\n",
    "    filelist = []\n",
    "    for dir in subject_dirs:\n",
    "        csv_files = [os.path.join(dir, csv) for csv in os.listdir(dir) if os.path.isfile(os.path.join(dir, csv)) and csv.endswith('.csv')]\n",
    "        for file in csv_files:\n",
    "            filelist.append(file)\n",
    "    \n",
    "    return filelist, subject_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read headers from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(fileList, subject_dirs):\n",
    "    master_csv_headers = []\n",
    "    for filename in fileList:\n",
    "        slash = filename.split(\"/\")\n",
    "        parts = slash[2]\n",
    "        subparts = parts.split(\"_\")\n",
    "        CIK = subparts[0]\n",
    "        report_type = subparts[1]\n",
    "        subsubpart = subparts[2].split('-')\n",
    "        report_year = subsubpart[1]    \n",
    "        df = pd.read_csv(filename,engine='python')\n",
    "        df['CIK'] = CIK\n",
    "        df['Reporting Type'] = report_type\n",
    "        df['Report Year'] = report_year\n",
    "        df.to_csv(filename, encoding='utf-8', index=False)\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            d_reader = csv.DictReader(f)\n",
    "            headers = d_reader.fieldnames\n",
    "            for header in headers:\n",
    "                master_csv_headers.append(header)\n",
    "            \n",
    "    return master_csv_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist, dirs = makefilelist(parent_dir)\n",
    "csv_headers = readCSV(filelist, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will only use the unqiue headers that we might require for our master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueHeaders(csv_headers):\n",
    "    return set(csv_headers)\n",
    "\n",
    "def chomp(list1):\n",
    "    list1 = [x.replace('\\n', '') for x in list1]\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our unique header values and taking a look on the kind of values we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837\n",
      "1837\n"
     ]
    }
   ],
   "source": [
    "final_headers = uniqueHeaders(csv_headers)\n",
    "print(len(uniqueHeaders(csv_headers)))\n",
    "lol = chomp(final_headers)\n",
    "print(len(lol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for different instances of similar headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this to test out the best conditions required to extract all the possible variants of each header in the unified CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter 0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for header in lol:\n",
    "    if 'des' in lol:\n",
    "        print(header)\n",
    "        counter+=1\n",
    "print (\"Counter\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will usse the above function as a way to guage how many similar ways are there to report a specific header and then merge all the occurances together to the master csv when we are conducting the joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the values present in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeValues(filelist):\n",
    "    for filename in filelist:\n",
    "        with open(filename, 'r') as f:\n",
    "            df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "            headers = list(df)\n",
    "            for header in headers:\n",
    "                if '000' in header:\n",
    "                    df[header] = df[header].astype(str) + '000'\n",
    "            print (df)\n",
    "            print(\"Filename\", filename)        \n",
    "            df.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normalizeValues(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate conditions for each type of Header "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will decide the final size of the unified CSV that we are going to generate for each time user searches for something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = [\"CIK\", \"Reporting Type\", \"Reporting Year\", \"Counterparty\", \"Notional Amount\", \"Reference Entity/Obligation\", \"Fixed Rate\", \"Expiration Date\", \"Appreciation/Depreciation\", \"Upfront Payments Paid/Received\", \"Implied Credit Spread\", \"Buy/Sell Protection\", \"Description\"]\n",
    "unified_csv = pd.DataFrame()\n",
    "CIK_csv = pd.DataFrame(columns=[\"CIK\"])\n",
    "Reporting_Type_csv = pd.DataFrame(columns=[\"Reporting Type\"])\n",
    "Reporting_Year_csv = pd.DataFrame(columns=[\"Reporting Year\"])\n",
    "Counterparty_csv = pd.DataFrame(columns=[\"Counterparty\"])\n",
    "Notional_Amount_csv = pd.DataFrame(columns=[\"Notional Amount\"])\n",
    "Reference_Entity_csv = pd.DataFrame(columns=[\"Reference Entity/Obligation\"])\n",
    "Fixed_Rate_csv = pd.DataFrame(columns=[\"Fixed Rate\"])\n",
    "Expiration_Date_csv = pd.DataFrame(columns=[\"Expiration Date\"])\n",
    "Appreciation_csv = pd.DataFrame(columns=[\"Appreciation/Depreciation\"])\n",
    "Upfront_Payments_csv = pd.DataFrame(columns=[\"Upfront Payments Paid/Received\"])\n",
    "Buy_Sell_csv = pd.DataFrame(columns=[\"Buy/Sell Protection\"])\n",
    "Description_csv = pd.DataFrame(columns=[\"Description\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the headers and merging them to a final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in filelist:\n",
    "    #Add flgas to detect changes\n",
    "    CIK_Flag = 0\n",
    "    ReportingType_Flag = 0\n",
    "    ReportingYear_Flag = 0\n",
    "    Counterparty_Flag = 0\n",
    "    NotionalAmount_Flag = 0\n",
    "    ReferenceEntity_Flag = 0\n",
    "    FixedRate_Flag = 0\n",
    "    ExpirationDate_Flag = 0\n",
    "    App_Dep_Flag = 0\n",
    "    Upfront_Payments_Flag = 0\n",
    "    Buy_Sell_Flag = 0\n",
    "    Description_Flag = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Reading and performing basic data cleaning \n",
    "    df = pd.read_csv(filename, engine='python', dtype=str)\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df = df.replace(r'\\t',' ', regex=True)\n",
    "    #Putting the list of headers in a list\n",
    "    headers = list(df)\n",
    "    \n",
    "    #Checking for CIK in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        #CIK NUMBER\n",
    "        if 'cik' in header.lower():\n",
    "            CIK_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                CIK_csv = CIK_csv.append({'CIK': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if CIK_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            CIK_csv = CIK_csv.append({'CIK': \"NaN\"}, ignore_index=True)\n",
    "            \n",
    "    #Checking for Reporting Type in the list if not append NaN for all rows        \n",
    "    for header in headers:    \n",
    "        if 'reporting' in header.lower():\n",
    "            ReportingType_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Type_csv = Reporting_Type_csv.append({'Reporting Type': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if ReportingType_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Type_csv = Reporting_Type_csv.append({'Reporting Type': \"NaN\"}, ignore_index=True)\n",
    "            \n",
    "    #Checking for Reporting Year in the list if not append NaN for all rows   \n",
    "    for header in headers:\n",
    "        if 'year' in header.lower():\n",
    "            ReportingYear_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Reporting_Year_csv = Reporting_Year_csv.append({'Reporting Year': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if ReportingYear_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reporting_Year_csv = Reporting_Year_csv.append({'Reporting Year': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Counterparty in the list if not append NaN for all rows\n",
    "    for header in headers:    \n",
    "        if 'counter' in header.lower() or 'party' in header.lower() or 'obligation' in header.lower():\n",
    "            Counterparty_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Counterparty_csv = Counterparty_csv.append({'Counterparty': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if Counterparty_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Counterparty_csv = Counterparty_csv.append({'Counterparty': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Notional Amount in the list if not append NaN for all rows\n",
    "    for header in headers:           \n",
    "        if 'notional' in header.lower() or 'amount' in header.lower() or 'value' in header.lower():\n",
    "            NotionalAmount_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Notional_Amount_csv = Notional_Amount_csv.append({'Notional Amount': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if NotionalAmount_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Notional_Amount_csv = Notional_Amount_csv.append({'Notional Amount': \"NaN\"}, ignore_index=True)\n",
    "                               \n",
    "    #Checking for Reference Entity/Obligation in the list if not append NaN for all rows             \n",
    "    for header in headers:              \n",
    "        if 'reference' in header.lower() or ('entity' in header.lower() or 'obligation' in header.lower()):\n",
    "            ReferenceEntity_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Reference_Entity_csv = Reference_Entity_csv.append({'Reference Entity/Obligation': i[1]}, ignore_index=True)\n",
    "   \n",
    "    if ReferenceEntity_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Reference_Entity_csv = Reference_Entity_csv.append({'Reference Entity/Obligation': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Fixed Rate in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'fixed' in header.lower() or 'rate' in header.lower():\n",
    "            FixedRate_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Fixed_Rate_csv = Fixed_Rate_csv.append({'Fixed Rate': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if FixedRate_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Fixed_Rate_csv = Fixed_Rate_csv.append({'Fixed Rate': \"NaN\"}, ignore_index=True)  \n",
    "            \n",
    "     #Checking for Expiration Date in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'date' in header.lower() or 'expiration' in header.lower() or 'termination' in header.lower() or 'maturity' in header.lower():\n",
    "            ExpirationDate_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Expiration_Date_csv = Expiration_Date_csv.append({'Expiration Date': i[1]}, ignore_index=True)\n",
    "                \n",
    "    if ExpirationDate_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Expiration_Date_csv = Expiration_Date_csv.append({'Expiration Date': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Appreciation Depreciation in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'appreciation' in header.lower() or 'depriciation' in header.lower():\n",
    "            App_Dep_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Appreciation_csv = Appreciation_csv.append({'Appreciation/Depreciation': i[1]}, ignore_index=True)\n",
    "    \n",
    "    if App_Dep_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Appreciation_csv = Appreciation_csv.append({'Appreciation/Depreciation': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Upfront Payments Paid/Received in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'upfront' in header.lower() or  ('payment' in header.lower() or 'premium' in header.lower()):\n",
    "            Upfront_Payments_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Upfront_Payments_csv = Upfront_Payments_csv.append({'Upfront Payments Paid/Received': i[1]}, ignore_index=True)\n",
    "    \n",
    "    if Upfront_Payments_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Upfront_Payments_csv = Upfront_Payments_csv.append({'Upfront Payments Paid/Received': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    #Checking for Buy/Sell Protection in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'buy' in header.lower() or  'sell' in header.lower() or 'protection' in header.lower():\n",
    "            Buy_Sell_Flag = 0\n",
    "            for i in df[header].iteritems():\n",
    "                Buy_Sell_csv = Buy_Sell_csv.append({'Buy/Sell Protection': i[1]}, ignore_index=True)\n",
    "    if Buy_Sell_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Buy_Sell_csv = Buy_Sell_csv.append({'Buy/Sell Protection': \"NaN\"}, ignore_index=True)\n",
    "    \n",
    "    #Checking for Description in the list if not append NaN for all rows\n",
    "    for header in headers:\n",
    "        if 'description' in header.lower():\n",
    "            Description_Flag = 1\n",
    "            for i in df[header].iteritems():\n",
    "                Description_csv = Description_csv.append({'Description': i[1]}, ignore_index=True)\n",
    "    if Description_Flag == 0:\n",
    "        for i in df[header].iteritems():\n",
    "            Description_csv = Description_csv.append({'Description': \"NaN\"}, ignore_index=True)         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's check the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buy/Sell Protection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17045</th>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17046</th>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17047</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17048</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17049</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17050</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17051</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17052</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17053</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17054</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17055</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17056</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17057</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17058</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17059</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17060</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17061</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17062</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17063</th>\n",
       "      <td>Sell***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17064</th>\n",
       "      <td>(1.000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17065</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17066</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17067</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17068</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17069</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17070</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17071</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17072</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17073</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17074</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17075 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Buy/Sell Protection\n",
       "0                     NaN\n",
       "1                     NaN\n",
       "2                     NaN\n",
       "3                     NaN\n",
       "4                     NaN\n",
       "5                     Buy\n",
       "6                     NaN\n",
       "7                     Buy\n",
       "8                     Buy\n",
       "9                     NaN\n",
       "10                    NaN\n",
       "11                    NaN\n",
       "12                    NaN\n",
       "13                    NaN\n",
       "14                    NaN\n",
       "15                    NaN\n",
       "16                    NaN\n",
       "17                    NaN\n",
       "18                    NaN\n",
       "19                    NaN\n",
       "20                    NaN\n",
       "21                    NaN\n",
       "22                    NaN\n",
       "23                    NaN\n",
       "24                    NaN\n",
       "25                    NaN\n",
       "26                    NaN\n",
       "27                    NaN\n",
       "28                    NaN\n",
       "29                    NaN\n",
       "...                   ...\n",
       "17045                Buy \n",
       "17046                Buy \n",
       "17047                 NaN\n",
       "17048                 NaN\n",
       "17049                 NaN\n",
       "17050                 NaN\n",
       "17051                 NaN\n",
       "17052                 NaN\n",
       "17053                 NaN\n",
       "17054                 NaN\n",
       "17055             Sell***\n",
       "17056             Sell***\n",
       "17057             Sell***\n",
       "17058             Sell***\n",
       "17059             Sell***\n",
       "17060             Sell***\n",
       "17061             Sell***\n",
       "17062             Sell***\n",
       "17063             Sell***\n",
       "17064             (1.000%\n",
       "17065                 NaN\n",
       "17066                 NaN\n",
       "17067                 NaN\n",
       "17068                 NaN\n",
       "17069                 NaN\n",
       "17070                 NaN\n",
       "17071                 NaN\n",
       "17072                 NaN\n",
       "17073                 NaN\n",
       "17074                 NaN\n",
       "\n",
       "[17075 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Buy_Sell_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_csv = CIK_csv.join(Reporting_Type_csv)\n",
    "unified_csv = unified_csv.join(Reporting_Year_csv)\n",
    "unified_csv = unified_csv.join(Counterparty_csv)\n",
    "unified_csv = unified_csv.join(Notional_Amount_csv)\n",
    "unified_csv = unified_csv.join(Reference_Entity_csv)\n",
    "unified_csv = unified_csv.join(Expiration_Date_csv)\n",
    "unified_csv = unified_csv.join(Appreciation_csv)\n",
    "unified_csv = unified_csv.join(Upfront_Payments_csv)\n",
    "unified_csv = unified_csv.join(Buy_Sell_csv)\n",
    "unified_csv = unified_csv.join(Description_csv)\n",
    "\n",
    "# Setting column type to str\n",
    "CIK_csv = CIK_csv.astype(str)\n",
    "Reporting_Type_csv = Reporting_Type_csv.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_csv.to_csv(\"final_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
